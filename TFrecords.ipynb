{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "test=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#helper functions for the features.\n",
    "def _int64_feature(value):\n",
    "#Wrapper for inserting int64 features into Example proto.\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "#Wrapper for inserting bytes features into Example proto.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "\n",
    "def image_example(image_string, label):\n",
    "#creates the example to write in the TFrecords file\n",
    "    image_shape = tf.image.decode_image(image_string)\n",
    "    feature = {\n",
    "      'height': _int64_feature(image_shape.shape[0]),\n",
    "      'width': _int64_feature(image_shape.shape[1]),\n",
    "      'depth': _int64_feature(image_shape.shape[2]),\n",
    "      'label': _int64_feature(label),\n",
    "      'image_raw': _bytes_feature(image_string),\n",
    "  }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def for_val(folder_path, record_file, val_percentage,numbers=10):\n",
    "#creates a randomised vector to choose files for validation data\n",
    "    file_num = 0\n",
    "    for i in range(numbers):\n",
    "        folder = i\n",
    "        data_folder = Path(folder_path + \"/%d\" %folder)\n",
    "        directory = os.fsencode(data_folder) \n",
    "        for file in os.listdir(directory):\n",
    "            file_num +=1\n",
    "    np.random.seed(26)\n",
    "    to_val = np.random.randint(file_num,size = int(file_num * val_percentage))\n",
    "    to_val = list(set(to_val))\n",
    "    to_val = np.sort(to_val)\n",
    "    lim = len(to_val)-1\n",
    "    return to_val, lim, file_num\n",
    "\n",
    "def create_val(folder_path, record_file, numbers=10,val_percentage=0.1):\n",
    "#creates the validation data tfrecords file\n",
    "    to_val, lim, _  = for_val(folder_path, record_file, val_percentage, numbers)\n",
    "    idx = 0\n",
    "    buffer_size=0\n",
    "    file_name = 'val_' + record_file\n",
    "    with tf.io.TFRecordWriter(file_name) as writer_val:\n",
    "        for i in range(numbers):\n",
    "            folder = i\n",
    "            data_folder = Path(folder_path + \"/%d\" %folder)\n",
    "            directory = os.fsencode(data_folder) \n",
    "            for file in os.listdir(directory):\n",
    "                filename = data_folder / os.fsdecode(file)\n",
    "                image_string = open(filename,'rb').read()\n",
    "                label = folder\n",
    "                tf_example = image_example(image_string, label)\n",
    "                if buffer_size == to_val[idx]:\n",
    "                    writer_val.write(tf_example.SerializeToString())\n",
    "                    if idx!=lim:\n",
    "                        idx +=1\n",
    "                buffer_size +=1\n",
    "\n",
    "def create_train(folder_path, record_file,numbers = 10, val_percentage=0.1):\n",
    "#creats the training data tfrecords file\n",
    "    to_val , lim, _  = for_val(folder_path, record_file, val_percentage, numbers)\n",
    "    idx = 0\n",
    "    buffer_size=0\n",
    "    file_name = 'train_' + record_file                    \n",
    "    with tf.io.TFRecordWriter(file_name) as writer_train:\n",
    "        for i in range(numbers):\n",
    "            folder = i\n",
    "            data_folder = Path(folder_path + \"/%d\" %folder)\n",
    "            directory = os.fsencode(data_folder) \n",
    "            for file in os.listdir(directory):\n",
    "                filename = data_folder / os.fsdecode(file)\n",
    "                image_string = open(filename,'rb').read()\n",
    "                label = folder\n",
    "                tf_example = image_example(image_string, label)\n",
    "                if buffer_size != to_val[idx]:\n",
    "                    writer_train.write(tf_example.SerializeToString())\n",
    "                else:\n",
    "                    if idx!=lim:\n",
    "                        idx +=1\n",
    "                buffer_size +=1\n",
    "                \n",
    "\n",
    "if test:\n",
    "    def create_test(folder_path, record_file,numbers = 10, val_percentage=0.1):\n",
    "    #creats the training data tfrecords file\n",
    "       #to_val , lim, _  = for_val(folder_path, record_file, val_percentage, numbers)\n",
    "        idx = 0\n",
    "        buffer_size=0\n",
    "        file_name = 'test_' + record_file                    \n",
    "        with tf.io.TFRecordWriter(file_name) as writer_test:\n",
    "            directory = os.fsencode(folder_path) \n",
    "            for file in os.listdir(directory):\n",
    "                filename = data_folder / os.fsdecode(file)\n",
    "\n",
    "            for i in range(numbers):\n",
    "                folder = i\n",
    "                data_folder = Path(folder_path + \"/%d\" %folder)\n",
    "                directory = os.fsencode(data_folder) \n",
    "                for file in os.listdir(directory):\n",
    "                    filename = data_folder / os.fsdecode(file)\n",
    "                    image_string = open(filename,'rb').read()\n",
    "                    label = folder\n",
    "                    tf_example = image_example(image_string, label)\n",
    "                    if buffer_size != to_val[idx]:\n",
    "                        writer_train.write(tf_example.SerializeToString())\n",
    "                    else:\n",
    "                        if idx!=lim:\n",
    "                            idx +=1\n",
    "                    buffer_size +=1\n",
    "    \n",
    "\n",
    "    \n",
    "record_file = 'minst.tfrecords'\n",
    "training_folder_path = 'mnist data/trainingSet/trainingSet'\n",
    "test_folder_path = 'mnist data/testSet/'\n",
    "create_val(training_folder_path, record_file)\n",
    "create_train(training_folder_path, record_file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 1) (32,)\n"
     ]
    }
   ],
   "source": [
    "def convert_back(data_type,buffer_size,record_file,val_percentage=0.1, channels =1, img_size = (28,28)):\n",
    "# converts the tfrecords files to images and labels and returns the parsed dataset\n",
    "    def _parse_image_function(example_proto):\n",
    "        # Parse the input tf.Example proto using the dictionary above.\n",
    "        # Create a dictionary describing the features.\n",
    "        image_feature_description = {\n",
    "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        image_features = tf.io.parse_single_example(example_proto,image_feature_description)\n",
    "        image_buffer = image_features['image_raw']\n",
    "        image = tf.image.decode_jpeg(image_buffer,channels = channels)\n",
    "        image = tf.image.convert_image_dtype(image,dtype=tf.float32)*(1. / 255)\n",
    "        image_shape = tf.stack([img_size[0],img_size[1],channels])\n",
    "        image = tf.reshape(image,image_shape)\n",
    "        label = tf.cast(image_features['label'],tf.uint8)\n",
    "        label = tf.squeeze(label)\n",
    "        return image,label\n",
    "    \n",
    "    batch_size = 32\n",
    "    num_parallel_batches = 2\n",
    "    if data_type == 'val':\n",
    "        buffer = int(buffer_size * val_percentage)\n",
    "    else:\n",
    "        buffer = buffer_size\n",
    "    raw_image_dataset = tf.data.TFRecordDataset(data_type+ '_' + record_file)\n",
    "    raw_image_dataset = raw_image_dataset.shuffle(buffer)\n",
    "    parsed_image_dataset = raw_image_dataset.map(_parse_image_function, num_parallel_calls = num_parallel_batches)\n",
    "    parsed_image_dataset = parsed_image_dataset.batch(batch_size)\n",
    "    parsed_image_dataset = parsed_image_dataset.prefetch(1)\n",
    "    return parsed_image_dataset\n",
    "\n",
    "\n",
    "record_file = 'minst.tfrecords'\n",
    "training_folder_path = 'mnist data/trainingSet/trainingSet'\n",
    "_, _, buffer_size = for_val(training_folder_path, record_file, val_percentage=0.1,numbers=10)\n",
    "val_ds = convert_back('val',buffer_size = buffer_size, record_file = record_file)\n",
    "train_ds = convert_back('train',buffer_size = buffer_size, record_file = record_file)\n",
    "\n",
    "#sanity check\n",
    "if not test:\n",
    "    for image, label in train_ds.take(1):\n",
    "        print(image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = 100\n",
    "kernels = (3,3)\n",
    "pools = (3,3)\n",
    "dense1 = 500\n",
    "dense2 = 250\n",
    "last_dense = 10\n",
    "dropout1 = 0.5\n",
    "dropout2 = 0.4\n",
    "\n",
    "#creating the model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Conv2D(filters, kernel_size=kernels, padding='same', input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D(pool_size=pools, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dense1, activation='relu'))\n",
    "model.add(Dropout(dropout1))\n",
    "model.add(Dense(dense2, activation='relu'))\n",
    "model.add(Dropout(dropout2))\n",
    "model.add(Dense(last_dense, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', metrics=['sparse_categorical_accuracy'], loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1188/1188 [==============================] - 7s 6ms/step - loss: 0.0967 - sparse_categorical_accuracy: 0.9699 - val_loss: 0.0654 - val_sparse_categorical_accuracy: 0.9818\n",
      "Epoch 2/10\n",
      "1188/1188 [==============================] - 5s 4ms/step - loss: 0.0895 - sparse_categorical_accuracy: 0.9717 - val_loss: 0.0715 - val_sparse_categorical_accuracy: 0.9776\n",
      "Epoch 3/10\n",
      "1188/1188 [==============================] - 5s 4ms/step - loss: 0.0877 - sparse_categorical_accuracy: 0.9715 - val_loss: 0.0660 - val_sparse_categorical_accuracy: 0.9818\n",
      "Epoch 4/10\n",
      "1188/1188 [==============================] - 5s 4ms/step - loss: 0.0841 - sparse_categorical_accuracy: 0.9737 - val_loss: 0.0605 - val_sparse_categorical_accuracy: 0.9830\n",
      "Epoch 5/10\n",
      "1188/1188 [==============================] - 5s 4ms/step - loss: 0.0823 - sparse_categorical_accuracy: 0.9742 - val_loss: 0.0580 - val_sparse_categorical_accuracy: 0.9825\n",
      "Epoch 6/10\n",
      "1188/1188 [==============================] - 5s 4ms/step - loss: 0.0728 - sparse_categorical_accuracy: 0.9768 - val_loss: 0.0586 - val_sparse_categorical_accuracy: 0.9830\n",
      "Epoch 7/10\n",
      "1188/1188 [==============================] - 5s 4ms/step - loss: 0.0757 - sparse_categorical_accuracy: 0.9761 - val_loss: 0.0597 - val_sparse_categorical_accuracy: 0.9811\n",
      "Epoch 8/10\n",
      "1188/1188 [==============================] - 5s 4ms/step - loss: 0.0735 - sparse_categorical_accuracy: 0.9768 - val_loss: 0.0497 - val_sparse_categorical_accuracy: 0.9860\n",
      "Epoch 9/10\n",
      "1188/1188 [==============================] - 5s 4ms/step - loss: 0.0691 - sparse_categorical_accuracy: 0.9783 - val_loss: 0.0536 - val_sparse_categorical_accuracy: 0.9850\n",
      "Epoch 10/10\n",
      "1188/1188 [==============================] - 5s 4ms/step - loss: 0.0713 - sparse_categorical_accuracy: 0.9768 - val_loss: 0.0498 - val_sparse_categorical_accuracy: 0.9835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2737fda8548>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=10, validation_data = val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
